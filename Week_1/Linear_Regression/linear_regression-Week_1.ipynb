{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Project (Linear Regression)\n",
    "## BMI to Life Expectancy\n",
    "### Single predictor variable\n",
    "\n",
    "In this section, you'll use linear regression to make prediction on life expectancy from [body mass index (BMI)](https://en.wikipedia.org/wiki/Body_mass_index) from birth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "In the above lines of code, we have imported pandas to read in the dataset.\n",
    "we import the LinearRegression model from sklearn because thats the method of prediction we are using.\n",
    "Lastly, we import matplotlib to visualise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign the dataframe to bmi_life_data var.\n",
    "bmi_life_data = pd.read_csv(\"bmi_and_life_expectancy.csv\")\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "#Fit the model and Assign it to bmi_life_model\n",
    "bmi_life_model = LinearRegression()\n",
    "bmi_life_model.fit(bmi_life_data[['BMI']], bmi_life_data[['Life expectancy']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "We read in bmi_and_life_expectancy.csv (conatins our dataset) using\n",
    "\n",
    "```python\n",
    "bmi_life_data = pd.read_csv(\"bmi_and_life_expectancy.csv\")\n",
    "```\n",
    "\n",
    "We then create a Linear Regression model and fit our data to the model\n",
    "We assign BMI to \"X\" and Life expectancy to \"Y\"\n",
    "\n",
    "Because we want to be able to predict life expantancy based on BMI, BMI is our training data, so it becomes X and our Target data is life expectancy so it becomes Y.\n",
    "\n",
    "Linear Regression model fit function works like the following\n",
    "model.fit(X(training data), Y(target data))\n",
    "\n",
    "```python\n",
    "bmi_life_model = LinearRegression()\n",
    "bmi_life_model.fit(bmi_life_data[['BMI']], bmi_life_data[['Life expectancy']])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25318521cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise Data\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('Life expectancy')\n",
    "plt.scatter(bmi_life_data[['BMI']], bmi_life_data[['Life expectancy']])\n",
    "plt.plot(bmi_life_data[['BMI']], bmi_life_model.predict(bmi_life_data[['BMI']]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Data\n",
    "In the example above, we visualise the data by creating a scatter plot of all our data points and then create a line of best fit\n",
    "\n",
    "The equation for the line is the following:\n",
    "$y=mx+b$ - single predictor variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 60.31564716]]\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction using the model. BMI value of 21.07931\n",
    "laos_life_exp = bmi_life_model.predict(21.07931)\n",
    "print(laos_life_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "lastly we made a prediction using our model with a BMI value of 21.07931.\n",
    "\n",
    "Our model should of predicted 60.31564716\n",
    "\n",
    "\n",
    "### Notes\n",
    "linear regression on works when the data is linear. Also outliers can have a dramatic effect. what we are looking for is a model that fits most of the data, most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References and support\n",
    "\n",
    "[Linear Regression - scikit-learn](ttp://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "[matplotlib - pyplot](http://matplotlib.org/users/pyplot_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston House Prices\n",
    "### multiple predictor variables\n",
    "\n",
    "in this section, we will use linear regression to predict the cost of a house based on 12 predicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Here we are importing the linear model from scikit-learn. We are also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from the the boston house-prices dataset \n",
    "boston_data = load_boston()\n",
    "x = boston_data['data']\n",
    "y = boston_data['target']\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "# Fit the model and Assign it to the model variable\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "We read in the boston housing data (conatins our dataset) using\n",
    "\n",
    "```python\n",
    "boston_data = load_boston()\n",
    "```\n",
    "\n",
    "We then create a Linear Regression model and fit our data to the model\n",
    "We assign boston_data['data'] to \"X\" and boston_data['target'] to \"Y\"\n",
    "\n",
    "Linear Regression model fit function works like the following\n",
    "model.fit(X(training data), Y(target data))\n",
    "\n",
    "```python\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "```\n",
    "\n",
    "The equation for the line is the following:\n",
    "$y=m_1x_1+m_2x_2+m_3x_3+\\cdots+m_nx_n+b$ - multi predictor variable\n",
    "\n",
    "Or it can be writen like the following\n",
    "$$\n",
    "y = (\\Sigma_{i=1}^n mx_i) + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 23.68420569]\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction using the model\n",
    "sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n",
    "                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n",
    "                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n",
    "# TODO: Predict housing price for the sample_house\n",
    "prediction = model.predict(sample_house)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "lastly we made a prediction using our model with a sample house:\n",
    "```python\n",
    "sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n",
    "                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n",
    "                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n",
    "```\n",
    "Our model should of predicted 23.68420569"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "[$y=mx+b$ - Explanation](http://www.math.com/school/subject2/lessons/S2U4L2GL.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hours of Study vs Test Scores\n",
    "#### Linear Regression and gradient descent\n",
    "\n",
    "x = the amount of hours studied and y = the test scores\n",
    "\n",
    "The equation for compute_error_for_line_given_points (computing the error of a line) is:\n",
    "\n",
    "$$\n",
    "Error_{(m,b)} =  \\frac 1N \\Sigma_{i=1}^n (y_i - (mx_i + b))^2\n",
    "$$\n",
    "\n",
    "so given m (slop) and b (Y intercept) compute the error of a line\n",
    "$$\n",
    "Error_{(m,b)}\n",
    "$$\n",
    "\n",
    "Sigma notation Calculating the sum of a set of values. i is the starting point, N is for every set (points).  \n",
    "$$\n",
    "\\Sigma_{i=1}^N\n",
    "$$\n",
    "\n",
    "We are calulating the difference in y values (essential y - y squared)\n",
    "$$\n",
    "(y_i - (mx_i + b))^2\n",
    "$$\n",
    "\n",
    "Calculate the average\n",
    "$$\n",
    "\\frac 1N\n",
    "$$\n",
    "\n",
    "The equations for calculating partial derivatives for m and b\n",
    "\n",
    "for m: $\\frac \\partial {\\partial m} = \\frac 2N \\Sigma_{i=1}^N - x_i(y_i(mx_i + b))$\n",
    "\n",
    "for b: $\\frac \\partial {\\partial b} = \\frac 2N \\Sigma_{i=1}^N - (y_i(mx_i + b))$\n",
    "\n",
    "The partial derivative\n",
    "\n",
    "for m: $\\frac \\partial {\\partial m}$\n",
    "\n",
    "for b: $\\frac \\partial {\\partial b}$\n",
    "\n",
    "Sigma notation Calculating the sum of a set of values. i is the starting point, N is for every set (points). \n",
    "\n",
    "for m: $\\Sigma_{i=1}^N$\n",
    "\n",
    "for b: $\\Sigma_{i=1}^N$\n",
    "\n",
    "The set calculation for m and b\n",
    "\n",
    "for m: $- x_i(y_i(mx_i + b))$\n",
    "\n",
    "for b: $- (y_i(mx_i + b))$\n",
    "\n",
    "\n",
    "The practical use of linear regression is to find the relationship between 2 value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting gradient descent at b = 0, m = 0 error = 5565.107834483211\n",
      "Running...\n",
      "After 10000 iterations b = 0.1512897437189306, m = 1.4824631965682806 error = 112.66584998492435\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "# Computes the error for the line with given points\n",
    "# this is our metric, that we want to minimise over time\n",
    "# we need to make this smaller\n",
    "# given (see parameters)\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    # intialize Error at 0\n",
    "    totalError = 0 # we dont have an error yet until it is calculated\n",
    "    \n",
    "    # for every point\n",
    "    for i in range(0, len(points)): # sigma notation (N in the equation)\n",
    "        # get the x value\n",
    "        x = points[i, 0] # x part of equation\n",
    "        # get the y value\n",
    "        y = points[i, 1] # y part of equation\n",
    "        # get the difference, square it, add it to the total\n",
    "        totalError += (y - (m * x + b)) **2 # this is the set part of equation\n",
    "    \n",
    "    # get the average\n",
    "    return totalError / float(len(points)) # 1/N (average part of equation)\n",
    "\n",
    "# gradient descent: local minima is where we are going\n",
    "# bowl, drop a ball in a bowl and at the point where the\n",
    "# ball stops is the optimal point\n",
    "# gradient means slope. the gradient acts as a compus. its\n",
    "# always going to point down. When the error is calulated its\n",
    "# going to tell us what direction we should be going\n",
    "# when lowest point is best, becuase thats when our error is smallest\n",
    "# when our error is the smallest that when we have the line of best fit\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    \n",
    "    # starting point for our gradiants\n",
    "    b_gradiant = 0\n",
    "    m_gradient = 0\n",
    "    \n",
    "    N = float(len(points))\n",
    "    \n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        \n",
    "        # direction with respect to b and m\n",
    "        # computing partial derivatives of our error function\n",
    "        # for every single point we are going to calculate the partial\n",
    "        # derivative with respect to b and respect to m. it will give us\n",
    "        # a direction for b and m\n",
    "        b_gradiant += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x)) + b_current)\n",
    "    \n",
    "    # update our b and m values using this partial derivatives\n",
    "    # the learning rate: defines the rate we are updating our b and m values\n",
    "    new_b = b_current - (learningRate * b_gradiant)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    \n",
    "    return [new_b, new_m]\n",
    "\n",
    "# gradient descent is used to reduce the error of the line we are drawing\n",
    "# used to reduce the error metric\n",
    "# given (see parameters)\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    # starting b and m\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    \n",
    "    # perform gradient descent\n",
    "    for i in range(num_iterations):\n",
    "        # update b and m with the new more accurate b and m\n",
    "        # by performing this gradient step\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "    \n",
    "    # return the optimal b and m\n",
    "    return [b, m]\n",
    "\n",
    "\n",
    "    \n",
    "def run():\n",
    "    \n",
    "    # Step 1 - collect our data\n",
    "    points = genfromtxt('data.csv', delimiter=',')\n",
    "    \n",
    "    # Step 2 - define our hyperparamters\n",
    "    \n",
    "    # How fast should our model converge? (When you get optimal results (line of best fit in LR))\n",
    "    learning_rate = 0.0001\n",
    "    \n",
    "    # y = mx + b (slope formula) (x and y are the points)\n",
    "    initial_b = 0 # y intercept (0 is our initial value (where we start))\n",
    "    initial_m = 0 # the slope (0 is our initial value (where we start))\n",
    "    num_interations = 1000 # how much we want to train our model (the number of iterations)\n",
    "    \n",
    "    # step 3 - train our model\n",
    "    \n",
    "    print('starting gradient descent at b = {0}, m = {1} error = {2}'.format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print('Running...')\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_interations)\n",
    "    print('After {0} iterations b = {1}, m = {2} error = {3}'.format(num_interations, b, m, compute_error_for_line_given_points(b, m, points)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if everything is correct, you should get a value for b around 0.15, m around 1.48, and an error value of around 112.66\n",
    "\n",
    "#### Resources\n",
    "[Github - linear regression live](https://github.com/llSourcell/linear_regression_live)\n",
    "\n",
    "[youtube - linear regression live](https://youtu.be/uwwWVAgJBcM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### How to make a Neural Network\n",
    "\n",
    "Below we will create a basic neural network only using numpy.\n",
    "\n",
    "We will use the below data set:\n",
    "\n",
    "| Training Set  | Input         | Output|\n",
    "| ------------- |:-------------:| -----:|\n",
    "| Example 1     |     0 0 1     |   0   |\n",
    "| Example 2     |     1 1 1     |   1   |\n",
    "| Example 3     |     1 0 1     |   1   |\n",
    "| Example 4     |     0 1 1     |   0   |\n",
    "|               |               |       |\n",
    "| New Situation |     1 0 0     |   ?   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import exp, array, random, dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "In the above lines of code, we have imported select functions from numpy needed for this neural network example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        # Seed the random number generator, so it generates the same numbers\n",
    "        # every time the program runs\n",
    "        random.seed(1)\n",
    "        \n",
    "        # We model a single neuron, with 3 input connections and 1 output connection\n",
    "        # We assign random weights to a 3 x 1 matrix, with values in the range -1 to 1\n",
    "        # and a mean of 0\n",
    "        self.synaptic_weights = 2 * random.random((3,1)) - 1\n",
    "    \n",
    "    # The sigmoid function, which describes an s shaped curve\n",
    "    # we pass the weighted sum of the inputs through this function\n",
    "    # to normalise them between 0 and 1\n",
    "    # will convert it to a probability between 0 and 1\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 /(1 + exp(-x))\n",
    "    \n",
    "    # gradient of the sigmoid curve\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1-x)\n",
    "    \n",
    "    # the real meat of the code. This performs the training\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "            # pass the training set through our neural net\n",
    "            output = self.predict(training_set_inputs)\n",
    "            \n",
    "            # calculate the error\n",
    "            error = training_set_outputs - output\n",
    "            \n",
    "            # this represents backpropagation - continiously updating weights via gradient desent\n",
    "            # multiply the error by the input and again by the gradient of the sigmoid curve\n",
    "            adjustment = dot(training_set_inputs.T, error * self.__sigmoid_derivative(output))\n",
    "            \n",
    "            # adjust the weights\n",
    "            self.synaptic_weights += adjustment\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        # pass inputs through our neural network (our single neuron)\n",
    "        return self.__sigmoid(dot(inputs, self.synaptic_weights))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class\n",
    "\n",
    "Here we implement a Neuralnetwork class.\n",
    "\n",
    "1. A Neural Network is an algorithm that learns to identify patterns in data\n",
    "\n",
    "2. Backpropagation is a technique to train a neural net by updating weights via gradient descent\n",
    "\n",
    "3. Deep learning = many layer neural net _ big data + big compute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting synaptic weights:\n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "New synaptic weights after training: \n",
      "[[ 9.67299303]\n",
      " [-0.2078435 ]\n",
      " [-4.62963669]]\n",
      "Considering new situation [1, 0, 0] -> ?: \n",
      "[ 0.99993704]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # initialise a single neuron neural network\n",
    "    neural_network = NeuralNetwork()\n",
    "    \n",
    "    print('Random starting synaptic weights:')\n",
    "    print(neural_network.synaptic_weights)\n",
    "    \n",
    "    # The training set. We have 4 examples, each consisting of 3 input values\n",
    "    # and 1 output value\n",
    "    training_set_input = array([[0,0,1], [1,1,1], [1,0,1], [0,1,1]])\n",
    "    training_set_outputs = array([[0,1,1,0]]).T\n",
    "    \n",
    "    # Since we define what category the output belongs to\n",
    "    # this is a classification task\n",
    "    # Train the neural network using a training set\n",
    "    # Do it 10,000 times and make small adjustments each time\n",
    "    neural_network.train(training_set_input, training_set_outputs, 10000)\n",
    "    \n",
    "    print('New synaptic weights after training: ')\n",
    "    print(neural_network.synaptic_weights)\n",
    "    \n",
    "    # Test the neural network with a new situation\n",
    "    print('Considering new situation [1, 0, 0] -> ?: ')\n",
    "    print(neural_network.predict(array([1, 0, 0])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "given a new set of data [1, 0, 0] our neural net should tell us that it has a high probability of producing a 1 or [0.99993704]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Classification Live Session\n",
    "\n",
    "tensorflow will be used for the following examples.\n",
    "\n",
    "We will be using tensorflow to classify housing prices\n",
    "\n",
    "Our data is located in the housing_prices.csv\n",
    "\n",
    "This will be supervised ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd # work with data as tables\n",
    "import numpy as np # use number matrices\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "Pandas to work with data as tables\n",
    "numpy to use number matrices\n",
    "matplotlib for creating and diplaying graphs\n",
    "tensorflow for machine learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>bathrooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1985.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1534.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1427.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1380.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1494.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     area  bathrooms\n",
       "0  2104.0        3.0\n",
       "1  1600.0        3.0\n",
       "2  2400.0        3.0\n",
       "3  1416.0        2.0\n",
       "4  3000.0        4.0\n",
       "5  1985.0        4.0\n",
       "6  1534.0        3.0\n",
       "7  1427.0        3.0\n",
       "8  1380.0        3.0\n",
       "9  1494.0        3.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1 is to load Data\n",
    "dataframe = pd.read_csv('housing_prices.csv') # read in a dataframe: Create dataframe object\n",
    "# a dataframe is an object in memory that pandas creates from a csv \n",
    "#file that we can then easily parse\n",
    "\n",
    "#removed columns that we dont care about: the features we dont care about\n",
    "# feature selection is its own displine. a good rule of thumb\n",
    "# is what features would you personaly use to make a prediction\n",
    "# we only care about the number of bathrooms and area\n",
    "dataframe = dataframe.drop(['index', 'price', 'sq_price'], axis=1)\n",
    "# axis is 1 because we are only going to use the first axis in the data set\n",
    "\n",
    "# we are only going to use the first 10 rows\n",
    "dataframe = dataframe[0:10]\n",
    "\n",
    "# print out the dataframe\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1985.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1534.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1427.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1380.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1494.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     area  bathrooms  y1  y2\n",
       "0  2104.0        3.0   1   0\n",
       "1  1600.0        3.0   1   0\n",
       "2  2400.0        3.0   1   0\n",
       "3  1416.0        2.0   0   1\n",
       "4  3000.0        4.0   0   1\n",
       "5  1985.0        4.0   1   0\n",
       "6  1534.0        3.0   0   1\n",
       "7  1427.0        3.0   1   0\n",
       "8  1380.0        3.0   1   0\n",
       "9  1494.0        3.0   1   0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2  - add labels: This is a classification problem\n",
    "# a good buy is a 1 and a bad buy is a 0\n",
    "dataframe.loc[:, ('y1')] = [1,1,1,0,0,1,0,1,1,1]\n",
    "\n",
    "# y2 is a negation of y1, it is the opposite. Y2 means we dont like a house\n",
    "dataframe.loc[:, ('y2')] = dataframe['y1'] == 0\n",
    "\n",
    "# turn TRUE/FALSE to 1s and 0s\n",
    "dataframe.loc[:, ('y2')] = dataframe['y2'].astype(int)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3 - prepare data for tensorflow (tensors)\n",
    "# tensors are a generic version of vectors and matrices\n",
    "# vector is a list of numbers (1D tesnor)\n",
    "# matrix is a list of list of numbers (2D tensor)\n",
    "#list of list of list of numbers (3D tensor)\n",
    "\n",
    "# convert features to input tensor \n",
    "inputX = dataframe.loc[:, ['area', 'bathrooms']].as_matrix()\n",
    "\n",
    "# convert labels to input tensor\n",
    "inputY = dataframe.loc[:, ['y1', 'y2']].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.10400000e+03,   3.00000000e+00],\n",
       "       [  1.60000000e+03,   3.00000000e+00],\n",
       "       [  2.40000000e+03,   3.00000000e+00],\n",
       "       [  1.41600000e+03,   2.00000000e+00],\n",
       "       [  3.00000000e+03,   4.00000000e+00],\n",
       "       [  1.98500000e+03,   4.00000000e+00],\n",
       "       [  1.53400000e+03,   3.00000000e+00],\n",
       "       [  1.42700000e+03,   3.00000000e+00],\n",
       "       [  1.38000000e+03,   3.00000000e+00],\n",
       "       [  1.49400000e+03,   3.00000000e+00]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputX # area and bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputY # y1 and y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4 - Write out our hyperparameters\n",
    "\n",
    "# controls the rate a which we learn. \n",
    "# defines how fast we reach convergince\n",
    "# Convergince is when our model is at optimal fit\n",
    "# optimnal fit is when the error is minimised \n",
    "# its alway a trade off of speed and accuracy\n",
    "learning_rate = 0.000001 \n",
    "\n",
    "# how many times we will be training\n",
    "training_epochs = 2000\n",
    "\n",
    "# how often do we want to display the process of training\n",
    "display_steps = 50\n",
    "\n",
    "# the number of samples\n",
    "n_samples = inputY.size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 5 - Create our computational graph/Neural network (they are the same thing)\n",
    "\n",
    "# create place holder for our feature tensor\n",
    "# for our feature input tensors\n",
    "# the Input is a float32\n",
    "# None means any numbers of examples\n",
    "# 2 because we have 2 features\n",
    "# placeholders are gateways for data. it is how we feed data to our computational graph\n",
    "x = tf.placeholder(tf.float32, [None,2])\n",
    "\n",
    "# create weights\n",
    "# for initial learning they should be always zero\n",
    "# in transfer learning they would not be (because they have already been trained)\n",
    "# 2x2 float matrix, that we'll keep updating\n",
    "# through the training process\n",
    "# in tensorflow Varibles hold and update parameters\n",
    "# they are in memory buffers containing tensors\n",
    "W = tf.Variable(tf.zeros([2,2]))\n",
    "\n",
    "# add biases - biases help our model fit better\n",
    "# example is b in y = mx + b (b is the bias)\n",
    "# we ant two bc we have 2 inputs\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "# multiply our weights by our inputs, first calculation\n",
    "# weights are how we govern how data flows in our computation graph\n",
    "# multiply input by weights and add biases\n",
    "# this is our computational step\n",
    "y_values = tf.add(tf.matmul(x, W), b)\n",
    "\n",
    "#softmax is another word for sigmoids\n",
    "# softmax normalizes our value. Converts\n",
    "# to a probablity so we can feed it to our output\n",
    "# apply softmax to value we just created\n",
    "#softmax is our activation function\n",
    "y = tf.nn.softmax(y_values)\n",
    "\n",
    "# feed in a matrix of labeles\n",
    "y_ = tf.placeholder(tf.float32, [None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 6 - Performing Training\n",
    "\n",
    "# Create our cost function, mean squared error\n",
    "# reduced sum, computes the sum of elements across dimension of a tensor\n",
    "cost = tf.reduce_sum(tf.pow(y_ - y, 2))/(2*n_samples)\n",
    "\n",
    "# Gradient descent - optimise our cost function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# initialize varibles and tensorflow session\n",
    "# init = tf.initialize_all_variables() # is deprecated and will be removed after 2017-03-02.\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step:  0000 cost= 0.114958666\n",
      "training step:  0050 cost= 0.109539941\n",
      "training step:  0100 cost= 0.109539866\n",
      "training step:  0150 cost= 0.109539807\n",
      "training step:  0200 cost= 0.109539732\n",
      "training step:  0250 cost= 0.109539673\n",
      "training step:  0300 cost= 0.109539606\n",
      "training step:  0350 cost= 0.109539531\n",
      "training step:  0400 cost= 0.109539464\n",
      "training step:  0450 cost= 0.109539405\n",
      "training step:  0500 cost= 0.109539315\n",
      "training step:  0550 cost= 0.109539248\n",
      "training step:  0600 cost= 0.109539196\n",
      "training step:  0650 cost= 0.109539129\n",
      "training step:  0700 cost= 0.109539054\n",
      "training step:  0750 cost= 0.109538987\n",
      "training step:  0800 cost= 0.109538913\n",
      "training step:  0850 cost= 0.109538853\n",
      "training step:  0900 cost= 0.109538779\n",
      "training step:  0950 cost= 0.109538712\n",
      "training step:  1000 cost= 0.109538652\n",
      "training step:  1050 cost= 0.109538577\n",
      "training step:  1100 cost= 0.109538510\n",
      "training step:  1150 cost= 0.109538436\n",
      "training step:  1200 cost= 0.109538361\n",
      "training step:  1250 cost= 0.109538302\n",
      "training step:  1300 cost= 0.109538235\n",
      "training step:  1350 cost= 0.109538175\n",
      "training step:  1400 cost= 0.109538101\n",
      "training step:  1450 cost= 0.109538034\n",
      "training step:  1500 cost= 0.109537959\n",
      "training step:  1550 cost= 0.109537885\n",
      "training step:  1600 cost= 0.109537825\n",
      "training step:  1650 cost= 0.109537765\n",
      "training step:  1700 cost= 0.109537683\n",
      "training step:  1750 cost= 0.109537624\n",
      "training step:  1800 cost= 0.109537557\n",
      "training step:  1850 cost= 0.109537482\n",
      "training step:  1900 cost= 0.109537408\n",
      "training step:  1950 cost= 0.109537348\n",
      "Optimization Finished!\n",
      "Training cost= 0.109537 W= [[  2.14149564e-04  -2.14149914e-04]\n",
      " [  5.12748193e-05  -5.12747974e-05]] b= [  1.19155184e-05  -1.19155284e-05]\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "# for every leanring epoc we have\n",
    "for i in range(training_epochs):\n",
    "    sess.run(optimizer, feed_dict={x: inputX, y_: inputY})\n",
    "    \n",
    "    # write out logs of training\n",
    "    if (i) % display_steps == 0:\n",
    "        cc = sess.run(cost, feed_dict={x: inputX, y_ : inputY})\n",
    "        print(\"training step: \", '%04d' % (i), \"cost=\", \"{:.9f}\".format(cc))\n",
    "\n",
    "print('Optimization Finished!')\n",
    "training_cost = sess.run(cost, feed_dict={x: inputX, y_ : inputY})\n",
    "print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.71125221,  0.28874779],\n",
       "       [ 0.66498977,  0.33501023],\n",
       "       [ 0.73657656,  0.26342347],\n",
       "       [ 0.64718789,  0.35281211],\n",
       "       [ 0.78335613,  0.2166439 ],\n",
       "       [ 0.70069474,  0.29930523],\n",
       "       [ 0.65866327,  0.34133676],\n",
       "       [ 0.64828628,  0.35171372],\n",
       "       [ 0.64368278,  0.35631716],\n",
       "       [ 0.65480113,  0.3451989 ]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(y, feed_dict={x: inputX})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "its saying all houses are a good buy 7/10 correct\n",
    "how to improve? add a hiden layer\n",
    "\n",
    "Response should be\n",
    "\n",
    "array([[ 0.71125221,  0.28874779],\n",
    "       [ 0.66498977,  0.33501023],\n",
    "       [ 0.73657656,  0.26342347],\n",
    "       [ 0.64718789,  0.35281211],\n",
    "       [ 0.78335613,  0.2166439 ],\n",
    "       [ 0.70069474,  0.29930523],\n",
    "       [ 0.65866327,  0.34133676],\n",
    "       [ 0.64828628,  0.35171372],\n",
    "       [ 0.64368278,  0.35631716],\n",
    "       [ 0.65480113,  0.3451989 ]], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "loglos function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
